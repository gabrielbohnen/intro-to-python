{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CODE - PlEASE RUN THIS ONCE WHEN YOU STARTUP YOUR CODESPACE\n",
    "\n",
    "# RUN TEST FILE\n",
    "%run test/week3_test.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 3 - Numpy, Pandas and DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy & Mathematics in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy, short for Numerical Python, is an essential library for scientific computing in Python. It provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "\n",
    "- Efficiency: NumPy arrays allow for faster operations than Python lists, especially for large data sets.\n",
    "- Functionality: NumPy provides a wide range of mathematical and statistical functions.\n",
    "- Convenience: It supports an array-oriented programming style, which simplifies many kinds of data manipulation tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Arrays\n",
    "The core of NumPy is the ndarray object, representing a multidimensional, homogeneous array of fixed-size items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# From a Python list\n",
    "arr = np.array([1, 2, 3])\n",
    "\n",
    "# Multidimensional array\n",
    "multi_arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(multi_arr)\n",
    "\n",
    "# Arrays of zeros and ones\n",
    "zeros = np.zeros((2, 3))\n",
    "ones = np.ones((3, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Operations\n",
    "NumPy provides a variety of operations that can be performed on arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise addition\n",
    "sum_arr = arr + arr\n",
    "\n",
    "# Element-wise multiplication\n",
    "prod_arr = arr * arr\n",
    "\n",
    "# Matrix product\n",
    "matrix_prod = np.dot(multi_arr, ones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Indexing and Slicing\n",
    "NumPy arrays can be sliced and indexed similar to Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing elements\n",
    "element = arr[0]\n",
    "\n",
    "# Slicing\n",
    "sub_array = multi_arr[0:2, 1:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful NumPy Functions\n",
    "NumPy provides many functions that are useful for statistical and mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.123233995736766e-17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean and standard deviation\n",
    "mean = np.mean(arr)\n",
    "std_dev = np.std(arr)\n",
    "\n",
    "# Sum and product\n",
    "arr_sum = np.sum(arr)\n",
    "arr_prod = np.prod(arr)\n",
    "\n",
    "# Transpose\n",
    "transposed = multi_arr.T\n",
    "\n",
    "# triginometric functions\n",
    "np.sin(np.pi / 2)\n",
    "np.cos(np.pi / 2) # <-- Observe FLOATING POINT ERROR - 6.123233995736766e-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Challenge Question 1**\n",
    "Catastrophic cancellation is a problem in numerical computing where significant digits of precision are lost due to the subtraction of two nearly equal numbers. This loss of precision can lead to highly inaccurate results, especially in floating-point computations.\n",
    "\n",
    "### Example of Catastrophic Cancellation in Python\n",
    "Let's consider a mathematical problem where catastrophic cancellation can occur. Suppose we want to compute the value of the expression sqrt(x + 1) - sqrt(x) for a very large x. Theoretically, as x becomes very large, this expression should approach zero. However, due to floating-point precision issues, the result can be inaccurate.\n",
    "\n",
    "Here's how this can be implemented and demonstrated in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstable_calculation(x):\n",
    "    return np.sqrt(x + 1) - np.sqrt(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation and Alternative Approach\n",
    "When subtracting two nearly equal numbers, many of the leading digits cancel out, and the difference is determined by the less significant digits, which are less accurately known. This leads to a result that can be significantly off from the true value.\n",
    "\n",
    "To avoid catastrophic cancellation, one approach is to reformulate the problem to avoid direct subtraction of nearly equal numbers. Using algebraic manipulation or an alternative formula that is mathematically equivalent but numerically more stable can often help.\n",
    "\n",
    "For the example above, we can use the mathematical identity:\n",
    "\n",
    "$$\n",
    "\\sqrt{a} - \\sqrt{b} = \\frac{(\\sqrt{a} - \\sqrt{b})(\\sqrt{a} + \\sqrt{b})}{\\sqrt{a} + \\sqrt{b}} = \\frac{a - b}{\\sqrt{a} + \\sqrt{b}}\n",
    "$$\n",
    "\n",
    "#### Applying this identity, we can rewrite the function in a more stable form:\n",
    "\n",
    "$$\n",
    "\\frac{(x + 1) - (x)}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_calculation(x):\n",
    "    # implement your stable function code here\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests to confirm your stable calculation function\n",
    "test_stable_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of stable vs unstable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a range of x values\n",
    "x_values = np.linspace(1, 1e12, 10000)\n",
    "\n",
    "# Calculate the absolute difference between stable and unstable calculations\n",
    "difference = [abs(stable_calculation(x) - unstable_calculation(x)) for x in x_values]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values, difference, label=\"Absolute Difference\", color='blue')\n",
    "plt.xscale('log')  # Using logarithmic scale for x-axis\n",
    "plt.yscale('log')  # Using logarithmic scale for y-axis\n",
    "plt.xlabel('x value')\n",
    "plt.ylabel('Absolute Difference')\n",
    "plt.title('Difference between Stable and Unstable Calculations')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is a great way to show that computational mathematics is not always just a lift and shift operation (ie. it's not just as simple as picking up the equation and typing it into python). There are a number of considerations that come into play such as numerical stability as seen in this Challenge Question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Challenge Question 2**\n",
    "Implement the following set of functions using NumPy:\n",
    "\n",
    "1. **create_array(x):**\n",
    "    - Takes a single integer `x` as input.\n",
    "    - Returns a NumPy 2D array of shape `(x, x)` filled with random integer values between 1 and 100 (inclusive).\n",
    "    \n",
    "2. **slice_array(arr, slice_size):**\n",
    "    - Takes as input a 2D NumPy array `arr` (created by `create_array`) and an integer `slice_size`.\n",
    "    - Returns the central region of `arr` with shape `(slice_size, slice_size)`.\n",
    "    - If there are any impossible computations then return `None`. For example, if `arr` isn't square, or if the user tries to find an even-sized central region inside an odd-sized array. \n",
    "\n",
    "3. **column_sum(arr):**\n",
    "    - Takes a 2D NumPy array `arr` as input.\n",
    "    - Returns a 1D NumPy array containing the sum of each column in `arr`.\n",
    "\n",
    "4. **normalise(arr):**\n",
    "    - Takes a 2D NumPy array `arr` as input.\n",
    "    - Returns a new array where each value is normalised in the range `[0, 1]`, using min-max normalisation. Hint: Look up how to calculate min-max normalisation\n",
    "    - If the min and max are the same, then return an array of ones of the same dimensions as the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(x:int) ->np.ndarray:\n",
    "    pass\n",
    "\n",
    "def slice_array(arr:np.ndarray, slice_size:int) -> np.ndarray:\n",
    "    pass\n",
    "\n",
    "def column_sum(arr:np.ndarray) -> np.ndarray:\n",
    "    pass\n",
    "\n",
    "def normalise(arr:np.ndarray) -> np.ndarray:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest 1 Failed/Skipped: Create 5x5 array. create_array function is not implemented\u001b[0m\n",
      "\u001b[91mTest 2 Failed/Skipped: Create 1x1 array. create_array function is not implemented\u001b[0m\n",
      "\u001b[91mTest 3 Failed/Skipped: Central 3x3 slice from 5x5 array (odd/odd). slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[91mTest 4 Failed/Skipped: Central 2x2 slice from 4x4 array (even/even). slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[92mTest 7 Passed: Odd array (5x5), even slice_size=2 -> None\u001b[0m\n",
      "\u001b[92mTest 8 Passed: Even array (4x4), odd slice_size=3 -> None\u001b[0m\n",
      "\u001b[92mTest 9 Passed: slice_size larger than array (5x5, slice=6) -> None\u001b[0m\n",
      "\u001b[92mTest 10 Passed: Non-square array (3x4) -> None\u001b[0m\n",
      "\u001b[91mTest 11 Failed/Skipped: Slice equal to array size (6x6, slice=6). slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[91mTest 12 Failed/Skipped: Central 4x4 from 8x8. slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[91mTest 13 Failed/Skipped: Central 5x5 from 9x9. slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[91mTest 14 Failed/Skipped: 1x1 array, slice=1 returns the single element. slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[92mTest 15 Passed: 2x2 array, slice=1 -> None (even/odd mismatch)\u001b[0m\n",
      "\u001b[92mTest 16 Passed: slice_size=0 should be invalid -> None (if enforced)\u001b[0m\n",
      "\u001b[92mTest 17 Passed: Negative slice_size -> None\u001b[0m\n",
      "\u001b[91mTest 18 Failed/Skipped: Float array central slice (5x5, slice=3). slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[91mTest 19 Warning: 1D input should be rejected, got None\u001b[0m\n",
      "\u001b[91mTest 20 Warning: 3D input should be rejected, got None\u001b[0m\n",
      "\u001b[91mTest 21 Failed/Skipped: Object dtype array central slice (5x5, slice=3). slice_array returned None but a valid slice was expected\u001b[0m\n",
      "\u001b[91mTest 5 Failed/Skipped: Sum of columns for 2x3 array. column_sum function is not implemented\u001b[0m\n",
      "\u001b[91mTest 22 Failed/Skipped: Single row. column_sum function is not implemented\u001b[0m\n",
      "\u001b[91mTest 23 Failed/Skipped: Single column. column_sum function is not implemented\u001b[0m\n",
      "\u001b[91mTest 24 Failed/Skipped: Zeros. column_sum function is not implemented\u001b[0m\n",
      "\u001b[91mTest 25 Failed/Skipped: Floats. column_sum function is not implemented\u001b[0m\n",
      "\u001b[91mTest 26 Failed/Skipped: Empty with 0 rows. column_sum function is not implemented\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_numpy_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_21280\\3838714237.py:212\u001b[39m, in \u001b[36mtest_numpy_challenge\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    210\u001b[39m test_create_array()\n\u001b[32m    211\u001b[39m test_slice_array()\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[43mtest_column_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m test_normalise()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_21280\\3838714237.py:158\u001b[39m, in \u001b[36mtest_numpy_challenge.<locals>.test_column_sum\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    156\u001b[39m expected_nan = np.array([np.nan, \u001b[32m4.0\u001b[39m])\n\u001b[32m    157\u001b[39m result = column_sum(arr_nan)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.isnan(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[32m1\u001b[39m] == \u001b[32m4.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNaN propagation test failed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGREEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mTest 27 Passed: NaN propagation in column_sum\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "test_numpy_challenge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Numpy Cheat Sheet**\n",
    "![numpy-cheat-sheet](../resources/images/week3/numpy_cheat_sheet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pandas DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a popular Python library for data manipulation and analysis. Its primary data structure is the DataFrame, a 2-dimensional labeled structure, ideal for handling various data types and complex data operations.\n",
    "\n",
    "### Key Features of Pandas DataFrames\n",
    "- Handling Different Data Types: Supports columns with diverse data types.\n",
    "- Size Mutability: Easy addition and deletion of columns.\n",
    "- Labeling Data: Clear labelling of rows and columns.\n",
    "- Advanced Data Operations: Offers a wide range of functions for data manipulation, including filtering, grouping, and pivoting.\n",
    "\n",
    "### Creating DataFrames\n",
    "DataFrames can be created from different data structures like dictionaries, lists, or numpy arrays. We can think of dataframes like tables or excel sheets that have rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Dataframe\n",
      "     Name  Age\n",
      "0  Alice   25\n",
      "1    Bob   30\n",
      "2  Chris   35\n",
      "3   John   48 \n",
      "\n",
      "\n",
      "List Dataframe\n",
      "     Name  Age\n",
      "0  Alice   25\n",
      "1    Bob   30\n",
      "2  Chris   35\n",
      "3   John   48\n"
     ]
    }
   ],
   "source": [
    "# from a dictionary\n",
    "data = {'Name': ['Alice', 'Bob', 'Chris', 'John'], 'Age': [25, 30, 35, 48]}\n",
    "dict_df = pd.DataFrame(data)\n",
    "\n",
    "# from a list\n",
    "names = ['Alice', 'Bob', 'Chris', 'John']\n",
    "ages = [25, 30, 35, 48]\n",
    "list_df = pd.DataFrame(list(zip(names, ages)), columns=['Name', 'Age'])\n",
    "\n",
    "print('Dictionary Dataframe\\n', dict_df, '\\n\\n')\n",
    "print('List Dataframe\\n', list_df)\n",
    "\n",
    "df = dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Data\n",
    "Pandas supports various file formats like CSV, Excel, and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "# df = pd.read_csv('data.csv') <- we don't want to run this code since we don't have a file called data.csv in this directory\n",
    "\n",
    "# write to csv\n",
    "# df.to_csv('output.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chris</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age\n",
       "0  Alice   25\n",
       "1    Bob   30\n",
       "2  Chris   35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # First 5 rows\n",
    "df.tail()  # Last 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Data with loc and iloc\n",
    "- loc: Selects data by labels/index.\n",
    "- iloc: Selects data by integer position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a single row by index\n",
    "row = df.loc[0]\n",
    "\n",
    "# Selecting a range of rows\n",
    "rows = df.iloc[1:3]\n",
    "\n",
    "# Selecting specific columns\n",
    "ages = df.loc[:, 'Age']\n",
    "name_age = df.iloc[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age\n",
      "2  Chris   35\n",
      "3   John   48\n"
     ]
    }
   ],
   "source": [
    "over_30 = df[df['Age'] > 30]\n",
    "\n",
    "print(over_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Deleting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age Country\n",
      "0  Alice   25     USA\n",
      "1    Bob   30     USA\n",
      "2  Chris   35     USA\n",
      "3   John   48     USA \n",
      "\n",
      "    Name  Age\n",
      "0  Alice   25\n",
      "1    Bob   30\n",
      "2  Chris   35\n",
      "3   John   48\n"
     ]
    }
   ],
   "source": [
    "df['Country'] = 'USA'  # Add new column\n",
    "\n",
    "print(df, '\\n')\n",
    "\n",
    "del df['Country']      # Delete column\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced DataFrame Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped DataFrame\n",
      "\n",
      "City: Brisbane\n",
      "    Name Department  Salary      City\n",
      "0  Alice    Finance   70000  Brisbane\n",
      "1    Bob         IT   65000  Brisbane \n",
      "\n",
      "City: Cairns\n",
      "    Name Department  Salary    City\n",
      "2  Chris         IT   80000  Cairns\n",
      "3   John    Finance   50000  Cairns \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct complimentry df\n",
    "employee_data = {\n",
    "    'Name': ['Alice', 'Bob', 'Chris', 'John'],\n",
    "    'Department': ['Finance', 'IT', 'IT','Finance'],\n",
    "    'Salary': [70000, 65000, 80000, 50000],\n",
    "    'City': ['Brisbane', 'Brisbane', 'Cairns', 'Cairns']\n",
    "    }\n",
    "\n",
    "employee_df = pd.DataFrame(employee_data)\n",
    "\n",
    "# group by \n",
    "grouped = employee_df.groupby('City')\n",
    "\n",
    "print('Grouped DataFrame\\n')\n",
    "\n",
    "# Iterate through groups\n",
    "for name, group in grouped:\n",
    "    print(f\"City: {name}\")\n",
    "    print(group, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Joining\n",
    "To be discussed in more detail next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Merged Dataframe\n",
      "     Name  Age Department  Salary      City\n",
      "0  Alice   25    Finance   70000  Brisbane\n",
      "1    Bob   30         IT   65000  Brisbane\n",
      "2  Chris   35         IT   80000    Cairns\n",
      "3   John   48    Finance   50000    Cairns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# merging and joining\n",
    "merged_df = pd.merge(df, employee_df, on='Name')\n",
    "\n",
    "print('\\n\\nMerged Dataframe\\n', merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Salary            Age\n",
      "            min     mean  mean\n",
      "City                          \n",
      "Brisbane  65000  67500.0  27.5\n",
      "Cairns    50000  65000.0  41.5\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Data on Grouping\n",
    "aggr_df = merged_df.groupby(\"City\").agg({'Salary':['min','mean'], 'Age':'mean'})\n",
    "print(aggr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot\n",
    "\n",
    "![pandas-pivot](../resources/images/week3/pandas_pivot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pivot tables\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pivot = \u001b[43mmerged_df\u001b[49m.pivot_table(values=[\u001b[33m'\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSalary\u001b[39m\u001b[33m'\u001b[39m], index=\u001b[33m'\u001b[39m\u001b[33mDepartment\u001b[39m\u001b[33m'\u001b[39m, aggfunc=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPivoted Dataframe\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m, pivot)\n",
      "\u001b[31mNameError\u001b[39m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# pivot tables\n",
    "pivot = merged_df.pivot_table(values=['Age', 'Salary'], index='Department', aggfunc='mean')\n",
    "\n",
    "print('\\nPivoted Dataframe\\n', pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operation are used by data engineers to merge, transform and mold certain data into various shapes for different purposes. The concepts are the same across python and SQL (Sructured Query Language) so the skills are all very transferrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pandas Data Wrangling Cheat Sheet**\n",
    "![pandas-data-wrangling-1](../resources/images/week3/pandas_datawrangling_1.PNG)\n",
    "![pandas-data-wrangling-2](../resources/images/week3/pandas_datawrangling_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Challenge Question 3**\n",
    "\n",
    "In this Challenge Question, you are provided with a scenario that involves two sets of data represented as Pandas DataFrames in Python. The first DataFrame, customers_df, contains customer data for a hypothetical energy company, while the second DataFrame energy_usage_df contains each customer's hourly energy usage data. The goal is to merge and analyze these datasets to gain insights into customer energy usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets setup some random data for our data frames\n",
    "\n",
    "# Sample customer data for 10 customers\n",
    "customer_data = {\n",
    "    'NMI': [f'NMI{100 + i}' for i in range(10)],\n",
    "    'Name': [f'Customer {i}' for i in range(1, 11)],\n",
    "    'Address': [f'{i} Some St' for i in range(100, 110)],\n",
    "    'Age': [20 + i for i in range(10)]\n",
    "}\n",
    "\n",
    "customers_df = pd.DataFrame(customer_data)\n",
    "\n",
    "# set np random seed which makes sure that random data is always the same for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate sample energy usage data for 10 customers\n",
    "hours = pd.date_range('2023-01-01', periods=24, freq='h')\n",
    "nmis = [f'NMI{100 + i}' for i in range(10)]\n",
    "energy_data = {\n",
    "    'NMI': np.repeat(nmis, 24),\n",
    "    'Hour': hours.tolist() * 10,\n",
    "    'kWh': np.random.rand(24 * 10) * 10  # Random kWh values for each hour\n",
    "}\n",
    "\n",
    "energy_usage_df = pd.DataFrame(energy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NMI</th>\n",
       "      <th>Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NMI100</td>\n",
       "      <td>Customer 1</td>\n",
       "      <td>100 Some St</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NMI101</td>\n",
       "      <td>Customer 2</td>\n",
       "      <td>101 Some St</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NMI102</td>\n",
       "      <td>Customer 3</td>\n",
       "      <td>102 Some St</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NMI103</td>\n",
       "      <td>Customer 4</td>\n",
       "      <td>103 Some St</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NMI104</td>\n",
       "      <td>Customer 5</td>\n",
       "      <td>104 Some St</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      NMI        Name      Address  Age\n",
       "0  NMI100  Customer 1  100 Some St   20\n",
       "1  NMI101  Customer 2  101 Some St   21\n",
       "2  NMI102  Customer 3  102 Some St   22\n",
       "3  NMI103  Customer 4  103 Some St   23\n",
       "4  NMI104  Customer 5  104 Some St   24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets list the customer data to see the rows columns\n",
    "\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NMI</th>\n",
       "      <th>Hour</th>\n",
       "      <th>kWh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NMI100</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>5.488135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NMI100</td>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>7.151894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NMI100</td>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>6.027634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NMI100</td>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>5.448832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NMI100</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>4.236548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      NMI                Hour       kWh\n",
       "0  NMI100 2023-01-01 00:00:00  5.488135\n",
       "1  NMI100 2023-01-01 01:00:00  7.151894\n",
       "2  NMI100 2023-01-01 02:00:00  6.027634\n",
       "3  NMI100 2023-01-01 03:00:00  5.448832\n",
       "4  NMI100 2023-01-01 04:00:00  4.236548"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets list the energy usage data to see the rows and columns\n",
    "energy_usage_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Customers Dataframe\n",
    "\n",
    "The customers_df DataFrame contains information about customers. Each row in this DataFrame represents a unique customer, with details about their identity and demographic information. Here's what each column represents:\n",
    "\n",
    "- NMI (National Meter Identifier): This is a unique identifier for each customer. It's a string that starts with 'NMI' followed by a number (e.g., 'NMI100', 'NMI101', etc.). This identifier is crucial for linking customers with their respective energy usage data.\n",
    "\n",
    "- Name: This column contains the name of the customer. In this dataset, customers are named in a sequence (e.g., 'Customer 1', 'Customer 2', etc.), indicating their order or position in the dataset.\n",
    "\n",
    "- Address: The address of each customer is listed here. Addresses are fictional and follow a numerical sequence (e.g., '100 Some St', '101 Some St', etc.). They provide a location context for each customer.\n",
    "\n",
    "- Age: This column shows the age of each customer. Ages are numeric values starting from 20 and increasing sequentially by 1 for each customer (e.g., 20, 21, 22, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Energy Usage Dataframe\n",
    "\n",
    "The energy_usage_df DataFrame contains energy usage data for each customer, detailed hour by hour. Each row in this DataFrame represents an hourly record of energy consumption for a customer. Here's the breakdown:\n",
    "\n",
    "- NMI: Just like in customers_df, this column contains the National Meter Identifier for each customer. It's used to link each energy usage record to the corresponding customer in customers_df.\n",
    "\n",
    "- Hour: This column contains datetime objects, each representing a specific hour of a day. For instance, if the date is '2023-01-01', the hourly breakdown will start from '2023-01-01 00:00:00' and go up to '2023-01-01 23:00:00', covering a full 24-hour period.\n",
    "\n",
    "- kWh (Kilowatt-hour): This column shows the amount of energy consumed during each specified hour. The values are numeric and represent the energy usage in kilowatt-hours. These values are randomly generated in the example, ranging between 0 and 10 kWh.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Note: questions marked with ** may be slightly more difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return the Data for One Customer**\n",
    "For the `energy_usage_df`, return the `kWh` values as a list between 10:00am and 1:00pm inclusive for NMI101. Store the results in a variable called `nmi101_filtered`. Hint: Look up .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the Dataframes\n",
    "Merge `customers_df` with `energy_usage_df` on the 'NMI' column and store it in a variable called `merged_df`. What does the merged DataFrame look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable called merged_df which holds the merged dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Total Energy Usage\n",
    "Calculate the total energy usage (kWh) for each customer in the merged DataFrame and assign it to the variable `total_energy_per_customer`\n",
    "\n",
    "Hint: when you group by a column you generally need to use an aggregation function like sum() or mean()\n",
    "\n",
    "example: `customer_salary = df.groupby('Name')['Salary'].sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a varaible called total_energy_per_customer\n",
    "# HINT: Use \"NMI\", not \"Name\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Who used the most Energy over the 24 Hour period\n",
    "\n",
    "Using your newly created variable `total_energy_per_customer` find out the name of the customer which used the most energy and assign it to the variable `highest_energy_usage_customer_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable called highest_energy_usage_customer_name and assign the name of the customer\n",
    "# who had the highest energy usage over the 24 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Energy Usage for Each hour of the day across all customers **\n",
    "\n",
    "Calculate the average energy usage for each hour of the day across all customers. Asign it to the variable `average_energy_per_hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable called average_energy_per_hour which stores the average energy usage across all customers for each our of \n",
    "# the day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate which hour had the highest usage\n",
    "Using your newly created `average_energy_per_hour` dataframe calculate which hour had the highest usage across all customers for the day. Assign it to the variable `highest_usage_hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable called highest_usage_hour which stores the hour which had the highest usage of electricity across\n",
    "# the entire day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Age-Energy Correlation Value **\n",
    "Calculate the correlation value between the age of the customer and the amount of energy usage and assign it to the variable `age_energy_correlation`. If we consider that a perfect correlation between two variables would be a value of `1`, do you think the value is significant enough to draw a causation between the two variables? That is can we accurately predict energy usage by age?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable called age_energy_correlation and calculate the correlation between a customers age and the energy they use.\n",
    "# HINT: use the merged_df table. You should return a single float value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest 1 Error: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[0m\n",
      "\u001b[91mTest 2 Failed: Testing merging of DataFrames. merged_df is not defined\u001b[0m\n",
      "\u001b[91mTest 3 Failed: Testing total energy usage calculation. total_energy_per_customer is not defined\u001b[0m\n",
      "\u001b[91mTest 4 Failed: Testing highest energy usage customer. highest_energy_usage_customer_name is not defined\u001b[0m\n",
      "\u001b[91mTest 5 Failed: Testing average energy usage per hour. average_energy_per_hour is not defined\u001b[0m\n",
      "\u001b[91mTest 6 Failed: Testing highest usage hour. highest_usage_hour is not defined\u001b[0m\n",
      "\u001b[91mTest 7 Failed: Testing age-energy correlation. age_energy_correlation is not defined\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# if you're a data wizard run some automated tests on your variables\n",
    "test_energy_analysis_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Challenge Question is a real world EQL example of how pandas is useful to solve our very complicated business problems. Except we are current scaling up to 2.5 million customers and reading their meters every 5 minutes. That's 720 Millions meter reading records stored in our databases everyday. We then calculate the price for each customer and send it off to the retailers to charge the customer. We estimate that we will store more data in the next two years then we have since the existance of both energex and ergon. Now that's BIGGGGG DATA!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9240d949b7e875368571ba59acc67192d2efbcc4561b3c6f94c83d7858e18732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
